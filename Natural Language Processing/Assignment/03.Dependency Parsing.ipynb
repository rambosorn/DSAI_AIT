{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rambo SORN_st123418"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Assignment - Dependency Parsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything should be done ON MY code, no new code.\n",
    "\n",
    "\n",
    "1. Read https://aclanthology.org/D14-1082.pdf and maybe just write one paragraph summary in your README.md in your github\n",
    "- github reading link: \n",
    "\n",
    "2. Do something called ablation study (meaning try to delete something so we know the impact of that deleted thing - very common in NLP)\n",
    "Recall that we have 18 word + 18 pos + 12 dep features\n",
    "Try to delete only the 12 dep features and check UAS\n",
    "Try to delete only the 18 pos features and check UAS\n",
    "3. Do another comparison study testing the embedding\n",
    "Chaky uses some embedding\n",
    "Try to use (1) glove embedding (smallest), (2) nn.Embedding (train from scratch) and compare with Chaky's embedding - on how it affects the UAS\n",
    "4. Do some testing, compare 2-3 sentences with spaCy and see whether our neural network gives the same dependency.\n",
    "\n",
    "Criteria:\n",
    "0: not done\n",
    "1: ok\n",
    "2: with comments/explanation like how Chaky does his tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improt Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sornr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm  #gimmick for progressbar when you train\n",
    "import pickle #saving and loading models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing function\n",
    "We gonna start with a class Parsing, representing a parser for each sentence. For each sentence, we need the stack, buffer, and the dependencies\n",
    "### Use code from Prof. class lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basically, it takes the current state of the buffer, stack, dependencies\n",
    "#tell us how SHIFT, LA, RA changes these three objects\n",
    "\n",
    "#just follow the professor code first \n",
    "#test it later\n",
    "\n",
    "class Parsing(object):\n",
    "    \n",
    "    #init stack, buffer, dep\n",
    "    def __init__(self, sentence):  \n",
    "        self.sentence = sentence     #['The', 'cat', 'sat]  #conll format which is already in the tokenized form\n",
    "        self.stack    = ['ROOT']\n",
    "        self.buffer   = sentence[:]  #in the beginning, everything is inside the buffer\n",
    "        self.dep      = []           #maintains a list of tuples of dep\n",
    "    \n",
    "    #parse function that tells me how shift, la, ra changes these three objects\n",
    "    def parse_step(self, transition):     #transition could be either S, LA, RA\n",
    "        if transition == 'S':\n",
    "            #get the top guy in the buffer and put in stack\n",
    "            head = self.buffer.pop(0)\n",
    "            self.stack.append(head)\n",
    "        elif transition == 'LA':  #stack = [ROOT, He, has] ==> append to dep (has, he) and then He is gone from the stack [ROOT, has]\n",
    "            dependent = self.stack.pop(-2)  #He\n",
    "            self.dep.append((self.stack[-1], dependent))  #(has, he)\n",
    "        elif transition == 'RA':\n",
    "            #can you guys try to this???\n",
    "            dependent = self.stack.pop()  #stack = [ROOT, has, control] ==> dep (has, control), control will be gone fromt he stack [ROOT, has]\n",
    "            self.dep.append((self.stack[-1], dependent))\n",
    "        else:\n",
    "            print(f\"Bad transition: {transition}\")\n",
    "    \n",
    "    #given some series of transition, it gonna for-loop the parse function\n",
    "    def parse(self, transitions):\n",
    "        for t in transitions:\n",
    "            self.parse_step(t)\n",
    "        return self.dep\n",
    "    \n",
    "    #check whether things are finished - no need to do anymore functions....\n",
    "    def is_completed(self):\n",
    "        return (len(self.buffer) == 0) and (len(self.stack) == 1)  #so buffer is empty and ROOT is the only guy in stack\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just follow the professor code first \n",
    "#test it later\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    dep = []  #all the resulting dep\n",
    "    \n",
    "    #init Parsing instance for each sentence in the batch\n",
    "    partial_parses = [Parsing(sentence) for sentence in sentences]  #in tokenized form\n",
    "    #Parsing(['The', 'cat', 'sat']), Parsing(['Chaky', 'is', 'mad'])\n",
    "    \n",
    "    unfinished_parses = partial_parses[:]\n",
    "    \n",
    "    #while we still have sentence\n",
    "    while unfinished_parses:  #if there are still a Parsing object\n",
    "    \n",
    "        #take a certain batch of sentence\n",
    "        minibatch = unfinished_parses[:batch_size] #number of Parsing object\n",
    "        \n",
    "        #create a dummy model to tell us what's the next transition for each sentence\n",
    "        transitions = model.predict(minibatch) \n",
    "        #transitions = [S, S, .....]\n",
    "        #minibatch   = [Parsing(sentence1), Parsing(sentence2)]\n",
    "        \n",
    "                \n",
    "        # for transition predicted this dummy model\n",
    "        for transition, partial_parse in zip(transitions, minibatch):\n",
    "            #parse step\n",
    "            #transition: S\n",
    "            #partial_parse: Parsing(sentence)\n",
    "            partial_parse.parse_step(transition)\n",
    "            \n",
    "        #remove any sentence is finish\n",
    "        unfinished_parses[:] = [p for p in unfinished_parses if not p.is_completed()]\n",
    "    \n",
    "    dep = [parse.dep for parse in partial_parses]\n",
    "    \n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(object):\n",
    "    def predict(self, partial_parses):\n",
    "        #partial_parses: list of Parsing instances\n",
    "        #first shift everything onto the stack, and then just do RA if the first word\n",
    "        #of the sentence is \"right\", otherwise, is \"left\"\n",
    "        return [(\"RA\" if pp.stack[1] == \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used English Penn Treebank dataset in CoNLL format.\n",
    "\n",
    "CoNLL is the conventional name for TSV formats in NLP (TSV - tab-separated values, i.e., CSV with as separator). It originates from a series of shared tasks organized at the Conferences of Natural Language Learning (hence the name)\n",
    "\n",
    "In CoNLL formats,\n",
    "\n",
    "- every word (token) is represented in one line\n",
    "- every sentence is separated from the next by an empty line\n",
    "- every column represents one annotation\n",
    "- There are many formats, in our case, our conll file has 10 columns, the important columns are:\n",
    "\n",
    "1: word\n",
    "4: pos\n",
    "6: head of the dependency\n",
    "7: type of dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(filename):\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        i = 0\n",
    "        word, pos, head, dep = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            i = i+1\n",
    "            wa = line.strip().split('\\t')  #['1', 'In', '_', 'ADP', 'IN', '_', '5', 'case', '_', '_']\n",
    "            #In <--------  5th guy\n",
    "            #     case\n",
    "            \n",
    "            if len(wa) == 10:  #if all the columns are there\n",
    "                word.append(wa[1].lower())\n",
    "                pos.append(wa[4])\n",
    "                head.append(int(wa[6]))\n",
    "                dep.append(wa[7])\n",
    "            \n",
    "            #the row is not exactly 10, it means new sentence\n",
    "            elif len(word) > 0:  #if there is somethign inside the word\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
    "                word, pos, head, dep = [], [], [], [] #clear word, pos, head, dep\n",
    "        \n",
    "        if len(word) > 0:  #if there is somethign inside the word\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
    "\n",
    "    return examples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"1. Loading data\")\n",
    "    dev_set = read_conll(\"dev.conll\")\n",
    "    test_set   = read_conll(\"test.conll\")\n",
    "    train_set   = read_conll(\"train.conll\")\n",
    "    \n",
    "    #make my dataset smaller because my mac cannot handle it\n",
    "    train_set = train_set[:1000]\n",
    "    dev_set   = dev_set[:500]\n",
    "    test_set  = test_set[:500]\n",
    "    \n",
    "    return train_set, dev_set, test_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a5e0dd1bb1c7782ce55b8cc0782cb511f0ece1dcdb1ee0c0eca7637fe7f72b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
